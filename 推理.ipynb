{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "108221192e3d475a80b62b806085c8f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from inferencer.inferencer import Inferencer\n",
    "from retriever.retriever_cone import ConeRetriever\n",
    "from retriever.retriever_topk import TopkRetriever\n",
    "from datareader.datareader import DatasetReader\n",
    "\n",
    "\n",
    "obj = Inferencer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj.prompt_template.inference_type =  'straight_forward' # 'deep_seek_r1'  # 'straight_forward'\n",
    "obj.generate_direct_prompt('/data/lhb/test-openicl-0.1.8/EHR_Base/results/readmission_prediction/randseed42/top3/qwen/top3_ice_idx.json', \n",
    "                           'top3_prompt_straight_forward.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [2:03:18<00:00, 36.99s/it]  \n"
     ]
    }
   ],
   "source": [
    "obj.generete_deepseek_r1_prompt('/data/lhb/test-openicl-0.1.8/EHR_Base/results/readmission_prediction/randseed42/random/random3_prompt_deepseek_r1.json',\n",
    "                                'random3_prompt_deepseek_r1_with_response.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [14:35,  4.38s/it]\n"
     ]
    }
   ],
   "source": [
    "obj.inference('/data/lhb/test-openicl-0.1.8/EHR_Base/top3_prompt_deepseek_r1_with_response.json',\n",
    "              'top3_res_deepseek_r1.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实验顺序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b43a1951f99c4bfc81b3a48937a94e71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from inferencer.inferencer import Inferencer\n",
    "from retriever.retriever_cone import ConeRetriever\n",
    "from retriever.retriever_topk import TopkRetriever\n",
    "from datareader.datareader import DatasetReader\n",
    "\n",
    "\n",
    "obj = Inferencer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [27:19,  8.20s/it]\n",
      "200it [28:44,  8.62s/it]\n"
     ]
    }
   ],
   "source": [
    "# random3_direct_res\n",
    "obj.inference('/data/lhb/test-openicl-0.1.8/EHR_Base/results/readmission_prediction/randseed42/random/random3_prompt_direct.json',\n",
    "              '/data/lhb/test-openicl-0.1.8/EHR_Base/results/readmission_prediction/randseed42/random/random3_res_direct.json')\n",
    "\n",
    "# random3_deepseek_res\n",
    "obj.inference('/data/lhb/test-openicl-0.1.8/EHR_Base/results/readmission_prediction/randseed42/random/random3_prompt_deepseek_r1_with_response.json',\n",
    "              '/data/lhb/test-openicl-0.1.8/EHR_Base/results/readmission_prediction/randseed42/random/random3_res_deepseek_r1.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [00:39,  5.08it/s]\n"
     ]
    }
   ],
   "source": [
    "# top3_bge_direct_res\n",
    "obj.inference('/data/lhb/test-openicl-0.1.8/EHR_Base/results/readmission_prediction/randseed42/top3/bge/top3_prompt_direct.json',\n",
    "              '/data/lhb/test-openicl-0.1.8/EHR_Base/results/readmission_prediction/randseed42/top3/bge/top3_res_direct.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "196it [31:25,  9.62s/it]\n"
     ]
    }
   ],
   "source": [
    "# top3_bge_deepseek_r1_res\n",
    "# obj.generete_deepseek_r1_prompt('/data/lhb/test-openicl-0.1.8/EHR_Base/results/readmission_prediction/randseed42/top3/bge/top3_prompt_deep_seek_r1.json',\n",
    "#                                 '/data/lhb/test-openicl-0.1.8/EHR_Base/results/readmission_prediction/randseed42/top3/bge/top3_prompt_deep_seek_r1_with_response.json',\n",
    "#                                 pause=195)\n",
    "obj.inference('/data/lhb/test-openicl-0.1.8/EHR_Base/results/readmission_prediction/randseed42/top3/bge/top3_prompt_deep_seek_r1_with_response_no56&70&148&194.json',\n",
    "              '/data/lhb/test-openicl-0.1.8/EHR_Base/results/readmission_prediction/randseed42/top3/bge/top3_res_deepseek_r1.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [00:39,  5.06it/s]\n"
     ]
    }
   ],
   "source": [
    "# top3_qwen_direct_res\n",
    "obj.inference('/data/lhb/test-openicl-0.1.8/EHR_Base/results/readmission_prediction/randseed42/top3/qwen/top3_prompt_direct.json',\n",
    "              '/data/lhb/test-openicl-0.1.8/EHR_Base/results/readmission_prediction/randseed42/top3/qwen/top3_res_direct.json', pause=195)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "192it [29:35,  9.24s/it]\n"
     ]
    }
   ],
   "source": [
    "# top3_qwen_deepseek_r1_res\n",
    "# obj.generete_deepseek_r1_prompt('/data/lhb/test-openicl-0.1.8/EHR_Base/results/readmission_prediction/randseed42/top3/qwen/top3_prompt_deep_seek_r1.json',\n",
    "#                                 '/data/lhb/test-openicl-0.1.8/EHR_Base/results/readmission_prediction/randseed42/top3/qwen/top3_prompt_deep_seek_r1_with_response.json',\n",
    "#                                 pause=195)\n",
    "obj.inference('/data/lhb/test-openicl-0.1.8/EHR_Base/results/readmission_prediction/randseed42/top3/qwen/top3_prompt_deep_seek_r1_with_response_no37&39&40&56&70&127&168&194.json',\n",
    "              '/data/lhb/test-openicl-0.1.8/EHR_Base/results/readmission_prediction/randseed42/top3/qwen/top3_res_deepseek_r1.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "194it [27:00,  8.36s/it]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 16.07 GiB. GPU 3 has a total capacity of 23.69 GiB of which 14.43 GiB is free. Including non-PyTorch memory, this process has 9.25 GiB memory in use. Of the allocated memory 8.49 GiB is allocated by PyTorch, and 465.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m obj\u001b[38;5;241m.\u001b[39mprompt_template\u001b[38;5;241m.\u001b[39minference_type \u001b[38;5;241m=\u001b[39m  \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstraight_forward\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;66;03m# 'deep_seek_r1'  # 'straight_forward'\u001b[39;00m\n\u001b[1;32m      3\u001b[0m obj\u001b[38;5;241m.\u001b[39mgenerate_direct_prompt(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/data/lhb/test-openicl-0.1.8/EHR_Base/results/readmission_prediction/randseed42/top10_cone3/bge/cone3_bge_idx.json\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m      4\u001b[0m                            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/data/lhb/test-openicl-0.1.8/EHR_Base/results/readmission_prediction/randseed42/top10_cone3/bge/cone3_prompt_direct.json\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/data/lhb/test-openicl-0.1.8/EHR_Base/results/readmission_prediction/randseed42/top10_cone3/bge/cone3_prompt_direct.json\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m              \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/data/lhb/test-openicl-0.1.8/EHR_Base/results/readmission_prediction/randseed42/top10_cone3/bge/cone3_res_direct.json\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m              \u001b[49m\u001b[43mpause\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m57\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/lhb/test-openicl-0.1.8/EHR_Base/inferencer/inferencer.py:112\u001b[0m, in \u001b[0;36mInferencer.inference\u001b[0;34m(self, input_path, output_path, pause)\u001b[0m\n\u001b[1;32m    110\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m'\u001b[39m], return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 112\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlogits\n\u001b[1;32m    114\u001b[0m next_token_logits \u001b[38;5;241m=\u001b[39m logits[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]  \u001b[38;5;66;03m# 只取最后一个 token's 的 logits\u001b[39;00m\n\u001b[1;32m    116\u001b[0m candidate_ans_logits_list \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/miniconda3/envs/openicl0.1.8/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/openicl0.1.8/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/openicl0.1.8/lib/python3.10/site-packages/accelerate/hooks.py:176\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    174\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 176\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/openicl0.1.8/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py:1186\u001b[0m, in \u001b[0;36mQwen2ForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1173\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\n\u001b[1;32m   1174\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1175\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1182\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m   1183\u001b[0m )\n\u001b[1;32m   1185\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m-> 1186\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlm_head\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1187\u001b[0m logits \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m   1189\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/openicl0.1.8/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/openicl0.1.8/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/openicl0.1.8/lib/python3.10/site-packages/accelerate/hooks.py:176\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    174\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 176\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/openicl0.1.8/lib/python3.10/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 16.07 GiB. GPU 3 has a total capacity of 23.69 GiB of which 14.43 GiB is free. Including non-PyTorch memory, this process has 9.25 GiB memory in use. Of the allocated memory 8.49 GiB is allocated by PyTorch, and 465.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# cone3_bge_direct_res\n",
    "obj.prompt_template.inference_type =  'straight_forward' # 'deep_seek_r1'  # 'straight_forward'\n",
    "obj.generate_direct_prompt('/data/lhb/test-openicl-0.1.8/EHR_Base/results/readmission_prediction/randseed42/top10_cone3/bge/cone3_bge_idx.json', \n",
    "                           '/data/lhb/test-openicl-0.1.8/EHR_Base/results/readmission_prediction/randseed42/top10_cone3/bge/cone3_prompt_direct.json')\n",
    "obj.inference('/data/lhb/test-openicl-0.1.8/EHR_Base/results/readmission_prediction/randseed42/top10_cone3/bge/cone3_prompt_direct.json',\n",
    "              '/data/lhb/test-openicl-0.1.8/EHR_Base/results/readmission_prediction/randseed42/top10_cone3/bge/cone3_res_direct.json',\n",
    "              pause=57)  # 跑到 194"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cone3_bge_deepseek_r1_res\n",
    "obj.prompt_template.inference_type =  'deep_seek_r1' # 'deep_seek_r1'  # 'straight_forward'\n",
    "obj.generate_direct_prompt('/data/lhb/test-openicl-0.1.8/EHR_Base/results/readmission_prediction/randseed42/top10_cone3/bge/cone3_bge_idx.json', \n",
    "                           '/data/lhb/test-openicl-0.1.8/EHR_Base/results/readmission_prediction/randseed42/top10_cone3/bge/cone3_prompt_deepseek_r1.json')\n",
    "obj.generete_deepseek_r1_prompt('/data/lhb/test-openicl-0.1.8/EHR_Base/results/readmission_prediction/randseed42/top10_cone3/bge/cone3_prompt_deepseek_r1.json', \n",
    "                           '/data/lhb/test-openicl-0.1.8/EHR_Base/results/readmission_prediction/randseed42/top10_cone3/bge/cone3_prompt_deepseek_r1_with_response.json')\n",
    "\n",
    "obj.inference('/data/lhb/test-openicl-0.1.8/EHR_Base/results/readmission_prediction/randseed42/top10_cone3/bge/cone3_prompt_deepseek_r1_with_response.json',\n",
    "              '/data/lhb/test-openicl-0.1.8/EHR_Base/results/readmission_prediction/randseed42/top10_cone3/bge/cone3_res_deepseek_r1.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cone3_qwen_direct_res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 检索器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Top10 retrive is going...: 100%|██████████| 200/200 [00:01<00:00, 148.41it/s]\n"
     ]
    }
   ],
   "source": [
    "from inferencer.inferencer import Inferencer\n",
    "from retriever.retriever_cone import ConeRetriever\n",
    "from retriever.retriever_topk import TopkRetriever\n",
    "from datareader.datareader import DatasetReader\n",
    "\n",
    "obj = TopkRetriever()\n",
    "\n",
    "res = obj.topk_retrive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([3192, 2586, 314, 1966, 2624, 4628, 1249, 4203, 1216, 1434], 200)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[0], len(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('top10_ice_idx.json', 'w', encoding='utf-8') as file:\n",
    "    json.dump(res, file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "815e8b6a6fd24c73b1646e4957368158",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from inferencer.inferencer import Inferencer\n",
    "from retriever.retriever_cone import ConeRetriever\n",
    "from retriever.retriever_topk import TopkRetriever\n",
    "from datareader.datareader import DatasetReader\n",
    "\n",
    "obj = ConeRetriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = obj.cone_retrive('/data/lhb/test-openicl-0.1.8/EHR_Base/results/readmission_prediction/randseed42/top10_cone3/bge/top10_ice_idx.json', 'cone3_bge_idx.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = obj.cone_retrive('/data/lhb/test-openicl-0.1.8/EHR_Base/results/readmission_prediction/randseed42/top10_cone3/qwen/top10_ice_idx.json', 'cone3_qwen_idx.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openicl0.1.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
